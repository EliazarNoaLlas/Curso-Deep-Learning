{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuGMchRQ_Itk"
      },
      "source": [
        "# **UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO**\n",
        "# **DEPARTAMENTO ACADÉMICO DE INFORMÁTICA**\n",
        "## **DEEP LEARNING**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgmIzERI_ikc"
      },
      "source": [
        "# **PRÁCTICA Nº 03**\n",
        "## **GRADIENTES Y DIFERENCIACIÓN AUTOMÁTICA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fY-jW0_zAW"
      },
      "source": [
        "**1.   OBJETIVOS**\n",
        "\n",
        "\n",
        "*   Conocer la diferenciación automática para calcular la derivada de una función\n",
        "*  Aplicar la diferenciación automática al entrenamiento de redes neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Kc8uBe2_mh"
      },
      "source": [
        "### **1.0 — Introducción**\n",
        "\n",
        "tf. GradientTape nos permite realizar un seguimiento de los cálculos de TensorFlow y calcular gradientes (con respecto a) algunas variables dadas.\n",
        "Por ejemplo, podríamos realizar un seguimiento de los siguientes cálculos y calcular gradientes de la siguiente manera:tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlhQBYc_3Qsp",
        "outputId": "8c300e6f-4a6d-43f2-91a5-3b944703ccd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlsGOrjM3JmN",
        "outputId": "7bb615fa-479c-4f13-f0e6-e293edd24ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(5.0)\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    y = x**3\n",
        "\n",
        "print(tape.gradient(y, x).numpy())\n",
        "\n",
        "del tape, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHK7fwfn30Jo"
      },
      "source": [
        "- Por defecto, no realiza un seguimiento de las constantes, por lo que \n",
        "debemos indicarle que: GradientTapetape.watch(variable)\n",
        "- Luego podemos realizar algún cálculo sobre las variables que estamos \n",
        "observando. El cálculo puede ser cualquier cosa, desde cubing, , hasta pasarlo a través de una red neuronal.x**3\n",
        "- Calculamos gradientes de un cálculo con una variabl. \n",
        "\n",
        "Nota, devuelve un archivo que puede convertir a formato ndarray con tape.gradient(target, sources)tape.gradientEagerTensor.numpy()\n",
        "\n",
        "Si en algún momento, queremos usar múltiples variables en nuestros cálculos, todo lo que tenemos que hacer es dar una lista o tupla de esas variables. Cuando optimizamos los modelos Keras, pasamos como nuestra lista de variables.tape.gradientmodel.trainable_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibrTFhm4kvt"
      },
      "source": [
        "### **1.1 — Observación automática de variables**\n",
        "\n",
        "Si fuera una variable entrenable en lugar de una constante, no habría necesidad de decirle a la cinta que la vea: observa automáticamente todas las variables entrenables.xGradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jvumGSn4w0R",
        "outputId": "e83ffdfb-d7f6-416d-8438-31ff644a20a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "108.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(6.0, trainable=True) # Error on: tf.constant(6.0) or tf.Variable(6.0, trainable=False)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x**3\n",
        "\n",
        "print(tape.gradient(y, x).numpy())\n",
        "\n",
        "del tape, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMPzKBV95Jcr"
      },
      "source": [
        "### **1.2 — watch_accessed_variables=Falso**\n",
        "Si no queremos ver todas las variables entrenables automáticamente, podemos establecer el parámetro de la cinta en: GradientTape(watch_accessed_variables=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69oXvJDz5R_h",
        "outputId": "b2554ba2-2750-44d8-d4fe-5aa853720fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = tf.Variable(3.0, trainable=True)\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    y = x**3\n",
        "\n",
        "print(tape.gradient(y, x))\n",
        "\n",
        "del tape, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fereeSpn5wKi"
      },
      "source": [
        "Deshabilitar nos da un buen control sobre qué variables queremos ver con watch_accessed_variables\n",
        "\n",
        "Si tiene muchas variables entrenables y no las está optimizando todas a la vez, es posible que desee deshabilitarlas para protegerse de errores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odiet-Oa53zQ"
      },
      "source": [
        "### **1.3 — Derivadas de orden superior**\n",
        "\n",
        "Si desea calcular derivadas de orden superior, puede utilizar anidados:GradientTapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHRnhfXC5-Xg",
        "outputId": "0c062abd-55c4-4c91-a46e-fdef40e4ef4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = tf.Variable(3.0, trainable=True)\n",
        "with tf.GradientTape() as tape1:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        y = x ** 3\n",
        "    order_1 = tape2.gradient(y, x)\n",
        "order_2 = tape1.gradient(order_1, x)\n",
        "\n",
        "print(order_2.numpy())\n",
        "\n",
        "del x, tape1, tape2, order_1, order_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_fjxkW6ErC"
      },
      "source": [
        "Las derivadas de orden superior son generalmente el único momento en que desea calcular gradientes dentro de un objeto. De lo contrario, ralentizará los cálculos a medida que observa cada cálculo realizado en el gradiente.GradientTapeGradientTape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEFo6MpM6IvI"
      },
      "source": [
        "### **1.4 — persistente=True**\n",
        "\n",
        "Si tuviéramos que ejecutar lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnlWQl9a6NL2",
        "outputId": "48c137f0-470f-4833-e2a6-44a975ee8a36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.0\n",
            "12.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "a = tf.Variable(6.0, trainable=True)\n",
        "b = tf.Variable(2.0, trainable=True)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    y1 = a ** 2\n",
        "    y2 = b ** 3\n",
        "                                                                                                                                                                                                                                                                                                                                                \n",
        "print(tape.gradient(y1, a).numpy())\n",
        "print(tape.gradient(y2, b).numpy())\n",
        "\n",
        "del a, b, tape, y1, y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vlxtm9U6Tea"
      },
      "source": [
        "Pero en realidad, llamar por segunda vez generará un error.tape.gradient\n",
        "\n",
        "Esto se debe a que inmediatamente después de llamar, libera toda la información almacenada en su interior con fines computacionales.tape.gradientGradientTape\n",
        "\n",
        "Si queremos evitar esto, podemos configurar persistent=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19vsEorR6eus"
      },
      "source": [
        "### **1.5 — stop_recording()**\n",
        "\n",
        "tape.stop_recording() pausa temporalmente la grabación de las cintas, lo que lleva a una mayor velocidad de cálculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cs1ttaQ6mEm",
        "outputId": "65ec58a2-c69d-4c8e-f353-9bd3cac6e3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(3.0, trainable=True)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x**3\n",
        "    with tape.stop_recording():\n",
        "        # Poner tape.gradient fuera de un bloque stop_recording, pero dentro del bloque de cinta, generará una advertencia sobre la ineficiencia.\n",
        "        print(tape.gradient(y, x).numpy())\n",
        "\n",
        "del tape, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6NqB5UN6sZT"
      },
      "source": [
        "En funciones largas, es más legible usar bloques varias veces para calcular gradientes en medio de una función, que calcular todos los gradientes al final de una función.stop_recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzYDGcLd6yVy",
        "outputId": "aa378441-0f9e-4aa6-d3ba-b84c608241c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.0\n",
            "12.0\n"
          ]
        }
      ],
      "source": [
        "a = tf.Variable(6.0, trainable=True)\n",
        "b = tf.Variable(2.0, trainable=True)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    y1 = a ** 2\n",
        "    with tape.stop_recording():\n",
        "        print(tape.gradient(y1, a).numpy())\n",
        "    \n",
        "    y2 = b ** 3\n",
        "    with tape.stop_recording():\n",
        "        print(tape.gradient(y2, b).numpy())\n",
        "                                                                                                                                                                                                                                                                                                                                                \n",
        "\n",
        "del a, b, tape, y1, y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssztJvAI64xo"
      },
      "source": [
        "El efecto es menos notable y posiblemente incluso lo contrario para un pequeño ejemplo, sin embargo, para una gran parte del código, creo que los bloques ayudan con creces a mejorar la legibilidad.stop_recording"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUltILpU6_Dq"
      },
      "source": [
        "### **1.6 — Otros métodos**\n",
        "\n",
        "Aunque no se entrará en detalles aquí, tiene algunos otros métodos útiles, que incluyen:GradientTape\n",
        "\n",
        "- jacobian: \"Calcula el jacobiano usando operaciones grabadas en el contexto de esta cinta.\"\n",
        "- batch_jacobian: \"Calcula y apila por ejemplo jacobianos\".\n",
        "- reset: \"Borra toda la información almacenada en esta cinta\".\n",
        "- watched_variables: \"Devuelve las variables observadas por esta cinta en orden de construcción\".\n",
        "Toda la información anterior citada de la documentación de GradientTape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtEPQtZQ7GEY"
      },
      "source": [
        "## **Usos avanzados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONjfZfli7K9z"
      },
      "source": [
        "### **2.0 — Regresión lineal**\n",
        "\n",
        "Para comenzar con los usos más avanzados de GradientTape, veamos un clásico \"¡Hola mundo!\" a ML: regresión lineal.\n",
        "\n",
        "Primero, comenzamos definiendo algunas variables y funciones esenciales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUhEhnZ27QOU",
        "outputId": "9d36da92-152b-46ad-d04b-da938822c6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y ≈ 0.027295438572764397x + 0.500281572341919\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#------- programa principal--------------\n",
        "\n",
        "puntos = np.genfromtxt(\"/content/data - data.csv\", delimiter=\",\")\n",
        "x = []\n",
        "# Función de pérdida\n",
        "def loss(real_y, pred_y):\n",
        "    return tf.abs(real_y - pred_y)\n",
        "\n",
        "# Datos de entrenamiento\n",
        "x_train = x\n",
        "y_train = np.asarray([(i*6.12)+1.35 for i in x_train]) # y = 6.12x+1.35\n",
        "\n",
        "# Variables entrenables\n",
        "a = tf.Variable(random.random(), trainable=True)\n",
        "b = tf.Variable(random.random(), trainable=True)\n",
        "\n",
        "# función de paso\n",
        "def step(real_x, real_y):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Hacer predicción\n",
        "        pred_y = a * real_x + b\n",
        "        # Cáclulo de pérdida\n",
        "        reg_loss = loss(real_y, pred_y)\n",
        "    \n",
        "    # Cacula gradientes\n",
        "    a_gradients, b_gradients = tape.gradient(reg_loss, (a, b))\n",
        "\n",
        "    # actualiza las variables\n",
        "    a.assign_sub(a_gradients * 0.001)\n",
        "    b.assign_sub(b_gradients * 0.001)\n",
        "\n",
        "# ciclo de entrenamiento\n",
        "for _ in range(10000):\n",
        "    step(x_train, y_train)\n",
        "\n",
        "print(f'y ≈ {a.numpy()}x + {b.numpy()}')\n",
        "\n",
        "del a, b, x_train, y_train, step, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOK0nO9c7abG"
      },
      "source": [
        "Luego, podemos seguir adelante y definir nuestra función de paso. La función de paso se ejecutará en cada época para actualizar las variables entrenables, a y b\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jwtJDda7lMJ"
      },
      "source": [
        "### **2.1 — Regresión polinómica**\n",
        "\n",
        "Podemos ampliar rápidamente el ejemplo anterior para trabajar con cualquier polinomio.\n",
        "\n",
        "Simplemente cambie las variables que estamos usando y la ecuación que estamos optimizando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNIw_0W77vK6",
        "outputId": "8c666751-cd2b-4289-e3fc-0ed7dc8c75f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y ≈ 5.961971282958984x^2 + 7.590671539306641x + 1.999351143836975\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# función de pérdida\n",
        "def loss(real_y, pred_y):\n",
        "    return tf.abs(real_y - pred_y)\n",
        "\n",
        "# Datos de entrenamiento\n",
        "x_train = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "y_train = np.asarray([6*i**2 + 8*i + 2 for i in x_train]) # y = 6x^2 + 8x + 2\n",
        "\n",
        "# variables entrenables\n",
        "a = tf.Variable(random.random(), trainable=True)\n",
        "b = tf.Variable(random.random(), trainable=True)\n",
        "c = tf.Variable(random.random(), trainable=True)\n",
        "\n",
        "# función de paso\n",
        "def step(real_x, real_y):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # hacer predicción\n",
        "        pred_y = a*real_x**2 + b*real_x + c\n",
        "        # Calcular pérdida\n",
        "        poly_loss = loss(real_y, pred_y)\n",
        "    \n",
        "    # Calcular gradientes\n",
        "    a_gradients, b_gradients, c_gradients = tape.gradient(poly_loss, (a, b, c))\n",
        "\n",
        "    # actualizar variables\n",
        "    a.assign_sub(a_gradients * 0.001)\n",
        "    b.assign_sub(b_gradients * 0.001)\n",
        "    c.assign_sub(c_gradients * 0.001)\n",
        "\n",
        "# Ciclo de entrenamiento\n",
        "for _ in range(10000):\n",
        "    step(x_train, y_train)\n",
        "\n",
        "print(f'y ≈ {a.numpy()}x^2 + {b.numpy()}x + {c.numpy()}')\n",
        "\n",
        "del a, b, x_train, y_train, step, loss\n",
        "\n",
        "#y = 6.002356052398682x^2 + 7.548577308654785x + 1.9996548891067505\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEbV3EkF8XbC"
      },
      "source": [
        "### **2.2 — Clasificación de MNIST**\n",
        "\n",
        "La regresión polinómica es divertida y todo, pero el verdadero problema es optimizar las redes neuronales.\n",
        "\n",
        "Afortunadamente para nosotros, poco tiene que cambiar de los ejemplos anteriores para hacer precisamente eso.\n",
        "\n",
        "Comenzamos siguiendo el procedimiento estándar, cargando los datos, preprocesándolos y configurando los hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwxEvdbnFMFM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnEPrtfeFthX"
      },
      "outputs": [],
      "source": [
        "# Load and pre-process training data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = (x_train / 255)\n",
        "x_test = (x_test / 255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXCU-SyMGX6e",
        "outputId": "f9fbd1c3-594f-4bbf-83a9-e564159c928f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "epochs = 25\n",
        "optimizer = SGD(lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLELog_uJYv6"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWN6qoRqL7KN"
      },
      "outputs": [],
      "source": [
        "def step(real_x, real_y):\n",
        "  with tf.GradientTape() as tape: # Record the gradient calculation\n",
        "    # Flatten x, [b, 28, 28] => [b, 784]\n",
        "    x = tf.reshape(x_train, (-1, 28*28))\n",
        "    # Step1. get output [b, 784] => [b, 10]\n",
        "    out = model(x)\n",
        "    #  [b] => [b, 10]\n",
        "    y_onehot = tf.one_hot(y_train, depth=10)\n",
        "    # Calculate squared error, [b, 10]\n",
        "    loss = tf.square(out-y_onehot)\n",
        "    # Calculate the mean squared error, [b]\n",
        "    loss = tf.reduce_sum(loss) / x.shape[0]\n",
        "    print('pérdida:',loss)\n",
        "  #Step3. Calculate gradients w1, w2, w3, b1, b2, b3\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  # Auto gradient calculation\n",
        "  #grads = tape.gradient(loss, model.trainable_variables)\n",
        "  # w' = w - lr * grad, update parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mkBIWznHKclm",
        "outputId": "f2c2cb23-522b-4416-9d42-cacf37336e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=pérdida: tf.Tensor(2.1264305, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(2.030654, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.9466203, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.872545, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.8069487, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.7486014, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.6965092, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.6498291, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.6078732, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.5700306, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.5357925, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.5047246, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.476453, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.4506584, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.4270588, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.4054135, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.3855071, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.3671602, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.3502067, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.334507, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.319936, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.3063897, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2937659, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2819756, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2709414, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2605925, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.250866, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2417086, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2330674, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2249005, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2171655, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2098262, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.2028518, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1962115, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1898757, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1838219, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.178027, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.172471, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1671365, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1620069, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1570667, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1523025, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1477026, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1432545, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1389484, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1347755, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1307253, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1267897, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1229621, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.119235, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1156018, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1120569, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1085955, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1052125, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.1019039, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0986649, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0954909, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0923789, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0893252, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0863272, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0833821, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0804869, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0776392, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0748374, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0720798, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0693642, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0666884, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0640509, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0614499, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.058884, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0563513, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0538507, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.051381, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0489414, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0465304, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.044147, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0417907, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0394607, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0371561, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0348762, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.03262, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0303872, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0281768, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0259885, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.023821, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0216742, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0195477, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0174408, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0153527, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.013283, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0112319, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0091989, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.007184, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0051864, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0032054, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(1.0012407, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9992921, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.99735934, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.99544203, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9935402, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9916535, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9897819, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9879249, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.98608214, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9842534, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9824384, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.98063695, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9788489, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.97707397, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9753124, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9735641, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9718286, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.97010577, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.96839535, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.96669686, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9650105, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9633364, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.961674, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9600231, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.958384, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.956756, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9551387, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.95353174, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9519362, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9503512, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.94877684, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.94721264, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9456581, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9441135, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9425789, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9410541, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9395392, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9380346, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9365395, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.93505406, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.933578, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9321112, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9306534, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.929205, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.92776567, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.92633504, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9249134, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.92350054, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.92209584, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9206997, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.91931224, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.91793346, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.916563, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9152009, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.913847, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.91250086, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9111626, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9098322, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9085093, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9071944, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9058871, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9045874, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9032949, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.9020096, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.90073174, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8994611, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.89819735, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8969406, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.895691, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8944483, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8932126, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.89198416, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8907625, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.88954765, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.88833946, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.887138, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.88594335, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8847552, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8835734, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8823977, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.88122845, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.88006556, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.878909, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.87775844, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.87661403, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8754758, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.87434375, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8732177, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8720976, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8709832, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8698745, shape=(), dtype=float32)\n",
            "pérdida: tf.Tensor(0.8687718, shape=(), dtype=float32)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-75ff17f9351f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbat_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-b2ff803291eb>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(real_x, real_y)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pérdida:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#Step3. Calculate gradients w1, w2, w3, b1, b2, b3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Auto gradient calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#grads = tape.gradient(loss, model.trainable_variables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstFirstOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstSecondOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;31m# No gradient skipping, so do the full gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGradAgainstSecondOnly\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6013\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6015\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   6016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "bat_per_epoch = math.floor(len(x_train) / batch_size)\n",
        "for epoch in range(epochs):\n",
        "    print('=', end='')\n",
        "    for i in range(bat_per_epoch):\n",
        "      step(x_train[i], y_train[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvxDI3vbP8Q7"
      },
      "source": [
        "### **TAREA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCrdexyQAB9"
      },
      "source": [
        "Utilizar la diferenciación automática para calcular los parámetros de la regresión lineal aplicada a los datos proporcionados y comparar con los resultados de la práctica 02."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "REGRESION LINEAL CON DIFERENCIACION AUTOMATICA_PRACTICA2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
