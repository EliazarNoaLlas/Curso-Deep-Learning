{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos la libreria de Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar la diferenciación automática para calcular los parámetros de la regresión lineal aplicada a los datos proporcionados y comparar con los resultados de la práctica 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #importamos\n",
    "print(tf.__version__) # version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regresión lineal - GradientTape(Automatico)**\n",
    "\n",
    "El siguiente codigo calcula los parametros de la funcion lineal en a y b de forma automatica utilizando  tf.GradientTape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función de pérdida (loss)\n",
    "def loss(real_y, pred_y):\n",
    "    return tf.reduce_mean(tf.square(real_y - pred_y))  # Mean Squared Error (MSE)\n",
    "\n",
    "# Leer datos desde el archivo CSV\n",
    "data = np.genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "N = len(data)\n",
    "x_train = data[:, 0]  # Columna 0: valores de entrada (características)\n",
    "y_train = data[:, 1]  # Columna 1: valores de salida (objetivos)\n",
    "print(\"Nro de datos leidos: \", N)\n",
    "\n",
    "# Variables entrenables\n",
    "a = tf.Variable(random.random(), trainable=True)\n",
    "b = tf.Variable(random.random(), trainable=True)\n",
    "\n",
    "# Función de paso\n",
    "def step(real_x, real_y, learning_rate):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Hacer predicción\n",
    "        pred_y = a * real_x + b\n",
    "        # Cálculo de pérdida\n",
    "        reg_loss = loss(real_y, pred_y)\n",
    "    \n",
    "    # Calcula gradientes\n",
    "    a_gradients, b_gradients = tape.gradient(reg_loss, (a, b))\n",
    "\n",
    "    # Actualiza las variables\n",
    "    a.assign_sub(a_gradients * learning_rate)\n",
    "    b.assign_sub(b_gradients * learning_rate)\n",
    "\n",
    "max_itera = 500000  # Número máximo de iteraciones permitidas\n",
    "ep = 0.0001  # Tolerancia o criterio de convergencia pero en este caso no se usa\n",
    "alfa = 0.0001  # Tasa de aprendizaje (learning rate)\n",
    "# Ciclo de entrenamiento\n",
    "for _ in range(max_itera):\n",
    "    step(x_train, y_train, alfa)\n",
    "\n",
    "print(f'y ≈ {a.numpy()}x + {b.numpy()}')\n",
    "\n",
    "#-- mostrar resultados\n",
    "print('Theta0: ',a)\n",
    "print('Theta1: ',b)\n",
    "\n",
    "\n",
    "# Graficar la función de la regresión lineal\n",
    "x_range = np.linspace(min(x_train), max(x_train), 100)\n",
    "y_pred = a.numpy() * x_range + b.numpy()\n",
    "\n",
    "# Graficamos los datos de entrenamiento y la regresión lineal\n",
    "plt.plot(x_train, y_train, 'ro', label='Datos de entrenamiento')\n",
    "plt.plot(x_range, y_pred, 'b-', label='Regresión lineal')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regresión Lineal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Liberar memoria\n",
    "del a, b, x_train, y_train, step, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regresión lineal - Minimos Cuadrados(de forma manual)**\n",
    "\n",
    "El siguiente codigo calcula los parametros de la funcion lineal en a y b de forma automatica utilizando  tf.GradientTape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nro de datos leidos:  100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\DeepLearning-Course\\Tarea_Practica_03.ipynb Celda 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m max_itera\u001b[39m=\u001b[39m \u001b[39m500000\u001b[39m \u001b[39m#-- nro maximo de iteraciones\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m#-- comienza el gradiente descendente\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m tetha0, tetha1 \u001b[39m=\u001b[39m gradient_descent(alfa,x,y,ep,max_itera)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m#-- mostrar resultados\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTheta0: \u001b[39m\u001b[39m'\u001b[39m,tetha0)\n",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\DeepLearning-Course\\Tarea_Practica_03.ipynb Celda 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#-- ciclo de iteraciones\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m convergio:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m#-- para cada ejemplo de entrenamiento calcular el gradiente (d/d_theta j(theta)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     grad0 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mN \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([(t0 \u001b[39m+\u001b[39m t1\u001b[39m*\u001b[39mx[i] \u001b[39m-\u001b[39m y[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     grad1 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mN \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([(t0 \u001b[39m+\u001b[39m t1\u001b[39m*\u001b[39mx[i] \u001b[39m-\u001b[39m y[i])\u001b[39m*\u001b[39mx[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m#-- actualizar los theta temporales\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\DeepLearning-Course\\Tarea_Practica_03.ipynb Celda 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#-- ciclo de iteraciones\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m convergio:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m#-- para cada ejemplo de entrenamiento calcular el gradiente (d/d_theta j(theta)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     grad0 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mN \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([(t0 \u001b[39m+\u001b[39m t1\u001b[39m*\u001b[39mx[i] \u001b[39m-\u001b[39m y[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     grad1 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mN \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([(t0 \u001b[39m+\u001b[39m t1\u001b[39m*\u001b[39mx[i] \u001b[39m-\u001b[39m y[i])\u001b[39m*\u001b[39mx[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/DeepLearning-Course/Tarea_Practica_03.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m#-- actualizar los theta temporales\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importacion de las bibliotecas para calculos numéricos \n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "alpha: Tasa de aprendizaje (learning rate) \n",
    "x: Lista de valores de entrada (características).\n",
    "y: Lista de valores de salida (objetivos).\n",
    "ep: Tolerancia o criterio de convergencia.\n",
    "max_iter: Número máximo de iteraciones permitidas.\n",
    "\"\"\"\n",
    "def gradient_descent(alpha, x, y, ep, max_iter):\n",
    "    convergio = False \n",
    "    iter = 0\n",
    "    N = len(x) #-- numero de ejemplos\n",
    "\n",
    "    #-- valores iniciales de theta\n",
    "    t0 = 0\n",
    "    t1 = 0\n",
    "\n",
    "    #-- error total, J(theta)\n",
    "    #-- apartir de los errores cuadraticos medios calculados\n",
    "    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(N)])\n",
    "\n",
    "    #-- ciclo de iteraciones\n",
    "    while not convergio:\n",
    "       \n",
    "        #-- para cada ejemplo de entrenamiento calcular el gradiente (d/d_theta j(theta)\n",
    "        grad0 = 1.0/N * sum([(t0 + t1*x[i] - y[i]) for i in range(N)]) \n",
    "        grad1 = 1.0/N * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(N)])\n",
    "\n",
    "        #-- actualizar los theta temporales\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "    \n",
    "        #-- actualizar los theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        #-- error cuadrado medio\n",
    "        e = sum( [(t0 + t1*x[i] - y[i])**2 for i in range(N)] ) \n",
    "\n",
    "        if abs(J-e) <= ep:\n",
    "            print ('Convergio con iteraciones: ', iter, '!!!')\n",
    "            convergio = True\n",
    "    \n",
    "        J = e   #-- actualizar error \n",
    "        iter += 1  #-- incrementa iteraciones\n",
    "    \n",
    "        if iter == max_iter: #-- si no converge \n",
    "            print ('Se excedió del máximo de iteraciones!')\n",
    "            convergio = True\n",
    "        # print (t0)\n",
    "        # print (t1)\n",
    "        # print ('---')\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "def graficar(tetha0,tetha1):\n",
    "  # Leemos los datos originales\n",
    "  points = np.genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "  x = points[:, 0]\n",
    "  y = points[:, 1]\n",
    "\n",
    "  # Recta real\n",
    "  # analizar y evaluar la recta real apartir de los datos\n",
    "  real_line = np.polyfit(x, y, 1)\n",
    "  real_line_x = np.linspace(min(x), max(x), 100)\n",
    "  real_line_y = np.polyval(real_line, real_line_x)\n",
    "\n",
    "  # Recta obtenida por el gradiente descendente\n",
    "  gradient_line_x = np.linspace(min(x), max(x), 100)\n",
    "  gradient_line_y = tetha0 + tetha1 * gradient_line_x\n",
    "\n",
    "  # Graficar los datos y las rectas\n",
    "  plt.scatter(x, y, label='Datos')\n",
    "  plt.plot(real_line_x, real_line_y, 'r', label='Recta Real')\n",
    "  plt.plot(gradient_line_x, gradient_line_y, 'g', label='Recta Gradiente Descendente')\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "#--------------- programa principal--------------\n",
    "\n",
    "points = genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "x = []\n",
    "y = []\n",
    "N = len(points)\n",
    "print(\"Nro de datos leidos: \", N)\n",
    "for i in range(N):\n",
    "  \n",
    "    x.append(points[i,0])\n",
    "    y.append(points[i,1])\n",
    "\n",
    "\n",
    "alfa = 0.0001  #-- learning rate -> tasa de aprendizaje\n",
    "ep = 0.0001 #-- tolerancia\n",
    "max_itera= 500000 #-- nro maximo de iteraciones\n",
    "\n",
    "#-- comienza el gradiente descendente\n",
    "tetha0, tetha1 = gradient_descent(alfa,x,y,ep,max_itera)\n",
    "#-- mostrar resultados\n",
    "print('Theta0: ',tetha0)\n",
    "print('Theta1: ',tetha1)\n",
    "\n",
    "# graficar la funcion\n",
    "graficar(tetha0,tetha1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
