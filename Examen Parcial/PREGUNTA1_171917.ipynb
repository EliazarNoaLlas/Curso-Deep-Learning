{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXTC-oT9ufbw"
      },
      "source": [
        "\n",
        "Nombre: Sandro Ramos Banda  \n",
        "\n",
        "Codigo: 171917\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ibu9taOuaVU"
      },
      "source": [
        "**OPTIMIZADOR ADAM**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lsgkGDaye9W0"
      },
      "outputs": [],
      "source": [
        "def initilization_Adam(params):\n",
        "    s = {}\n",
        "    v = {}\n",
        "    for i in range(len(params)//2 ):\n",
        "\n",
        "        v[\"dW\" + str(i)] = np.zeros(params[\"W\" + str(i)].shape)\n",
        "        v[\"db\" + str(i)] = np.zeros(params[\"b\" + str(i)].shape)\n",
        "\n",
        "        s[\"dW\" + str(i)] = np.zeros(params[\"W\" + str(i)].shape)\n",
        "        s[\"db\" + str(i)] = np.zeros(params[\"b\" + str(i)].shape)\n",
        "    return v, s\n",
        "    \n",
        "def update_params_with_Adam(params, grads,v, s, beta1,beta2, learning_rate,t):\n",
        "    epsilon = pow(10,-8)\n",
        "    v_corrected = {}                         \n",
        "    s_corrected = {} \n",
        "    # grads has the dw and db parameters from backprop\n",
        "    # params  has the W and b parameters which we have to update \n",
        "    for l in range(len(params) // 2 ):\n",
        "        # HERE WE COMPUTING THE VELOCITIES \n",
        "\n",
        "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads['dW' + str(l)]\n",
        "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1 - beta1) * grads['db' + str(l)]\n",
        "\n",
        "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - np.power(beta1, t))\n",
        "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - np.power(beta1, t))\n",
        "\n",
        "\n",
        "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + (1 - beta2) * np.power(grads['dW' + str(l)], 2)\n",
        "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + (1 - beta2) * np.power(grads['db' + str(l)], 2)\n",
        "\n",
        "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - np.power(beta2, t))\n",
        "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - np.power(beta2, t))\n",
        "\n",
        "        params[\"W\" + str(l)] = params[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / np.sqrt(s_corrected[\"dW\" + str(l)] + epsilon)\n",
        "        params[\"b\" + str(l)] = params[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / np.sqrt(s_corrected[\"db\" + str(l)] + epsilon)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsUdDGkUzLEd",
        "outputId": "36f7af97-8853-43cd-ccfe-852dd8af9944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se excedió del máximo de iteraciones!\n",
            "Theta0:  0.7314663340263162\n",
            "Theta1:  -0.9086658067783905\n"
          ]
        }
      ],
      "source": [
        "#-- ADAM\n",
        "from numpy import *\n",
        "def update_params_with_Adam(alpha, x, y, ep, max_iter):\n",
        "    convergio = False\n",
        "    iter = 0\n",
        "    N = len(x) #-- número de ejemplos\n",
        "    #-- valores iniciales de theta\n",
        "    t0 = 0\n",
        "    t1 = 0\n",
        "    #-- error total, J(theta)\n",
        "    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(N)])\n",
        "    #-- ciclo de iteraciones\n",
        "    while not convergio:\n",
        "      #-- para cada ejemplo de entrenamiento calcular el gradiente (d/d_theta)j(theta)\n",
        "      grad0 = 1.0/N * sum([(t0 + t1*x[i] - y[i]) for i in range(N)])\n",
        "      grad1 = 1.0/N * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(N)])\n",
        "      #-- actualizar las thetas temporales\n",
        "      temp0 = t0 - alpha * grad0\n",
        "      temp1 = t1 - alpha * grad1\n",
        "      #-- actualizar las theta\n",
        "      t0 = temp0\n",
        "      t1 = temp1\n",
        "      #-- calcula el error cuadrado medio\n",
        "      e = sum( [(t0 + t1*x[i] - y[i])**2 for i in range(N)] )\n",
        "      if abs(J-e) <= ep:\n",
        "        print ('Convergió con iteraciones: ', iter, '!!!')\n",
        "        convergio = True\n",
        "      J = e #-- actualizar error\n",
        "      iter += 1 #-- incrementa iteraciones\n",
        "      if iter == max_iter: #-- si no converge\n",
        "        print ('Se excedió del máximo de iteraciones!')\n",
        "        convergio = True\n",
        "    return t0,t1 #-- devuelve los valores calculados de theta0 y theta1\n",
        "#------- programa principal--------------\n",
        "x=[1,2,3,4]\n",
        "y=[0,-1,-2,-3]\n",
        "#-- parametros iniciales\n",
        "alfa= 0.01 #-- learning rate\n",
        "ep = 0.00001 #-- tolerancia\n",
        "max_itera= 1000 #-- nro maximo de iteraciones\n",
        "#-- comienza el gradiente descendente\n",
        "tetha0, tetha1 = update_params_with_Adam(alfa,x,y,ep,max_itera)\n",
        "#-- mostrar resultados\n",
        "print('Theta0: ',tetha0)\n",
        "print('Theta1: ',tetha1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPhrobXwzEez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uohb8n_dzCUl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
